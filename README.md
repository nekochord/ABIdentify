# 用python實作簡單的深度學習
## (一)介紹Deep Learn
相信大家應該都看過威爾史密斯主演的電影"機械公敵"吧。

在電影當中的世界，人工智慧高度發展，而人類的生活也因此充滿了許多
高智慧的機器人。這些機器人能夠做到許多的事，從日常生活中的購物、
做菜到接送上下班，皆能夠輕鬆完成。

但是在現實生活中，我們是否能夠真正地做出一台會思考的機器?
為了這個疑問，電腦科學發展出了"人工智慧"這個領域，
而"deep learn"則是人工智慧領域當中的一個分支
## (二)Deep Learn Network 的核心概念
假設我們現在有一張貓的圖片~![](https://i.imgur.com/pAtPYtH.jpg)
我們要如何讓電腦知道這張圖片是貓?

我們當然可以預先告知電腦有關貓的所有特徵，讓電腦進行判斷。

但是"深度學習"提供了我們另外的方法，利用提供"大量"貓的照片
來讓電腦自己找到貓的特徵。

> 談到這裡，附上一個對我這次實作幫助最大的影片。
影片的脈絡十分清楚，可以補充我遺漏的許多重要概念XD
[一天搞懂深度學習](https://www.youtube.com/watch?v=ZrEsLwCjdxY)
[影片中的投影片](http://www.slideshare.net/tw_dsconf/ss-62245351)

但是我們要如何讓電腦自己去找尋貓的特徵呢?

要知道這個問題就必須要談到深度學習的核心概念"Neural Network神經網路"
### 介紹神經元Neuron:
Neuron的主要概念就是模仿真實神經細胞作用的模式。

![](https://i.imgur.com/t1fSGex.png)


其中的 w1, w1, w3... 以及 b 就是神經元的權重
k1, k2, k3...是輸入值
A是激活函數
寫成公式就是以下的樣子
![](https://i.imgur.com/9CrQ40T.jpg)


(除了b以外的參數都可以依照自己的需求增加或減少。但是輸出都只會有一個值。)

至於A這個激活函數，則是有許多不同的種類。sigmoid、ReLu...等
本篇採用最經典的(=最老的)sigmoid函數，函數值介於0到1之間。
![](https://i.imgur.com/gj3EWQz.jpg)
函數圖形如下:
![](https://i.imgur.com/ZzUyyJy.jpg)

有了這樣的模型之後，我們就可以透過修改神經元的權重來控制這個神經元的輸出。
### 介紹神經元層Layer、神經網路

透過組合N個神經元(有M個權重)，就可以得到一個輸入為M、輸出為N的神經層。
![](https://i.imgur.com/CjlgFEK.jpg)

透過組合多個神經元層，就可以得到一個深度學習網路。
![](https://i.imgur.com/pZh8PW8.png)

談到這裡，你可能會有一個疑問。
好，我有了一個神經網路。但它到底是什麼?
* 答案: 一個有超多參數的複雜"方程式"

國中時我們學過線性方程式:
![](https://i.imgur.com/M9yBH1d.jpg)
僅僅改變a跟b兩個參數，我們可以造出各種不一樣的直線方程式

如果把"看"當成f()，x是我們的貓咪照片。
那麼透過修改參數，我們就可以找到正確的f
使得f(x)="這是一隻喵!"

而正確的神經網路方程式F應該會長成這樣:
![](https://i.imgur.com/fH9jI9c.jpg)

當然你可能會覺得有其他的方式可以建構出這種具有大量參數的方程式。
那為什麼要特地使用這種架構?
關於這個問題，我也不知道XD。
但是，為了想要揣摩人腦的思考模式，而從神經元的概念開始構想，我認為是十分直覺的。
>補充:
>如果你還是覺得無法理解這部分的內容
>我會建議你可以先用紙筆畫一個簡單的模型然後寫出方程式，或是直接進入實作。
>如果你有時間，建議可以觀看前面提到的影片。
>[一天搞懂深度學習](https://www.youtube.com/watch?v=ZrEsLwCjdxY)
## (三)如何建構一個深度學習網路

前面我們提到只要找到正確的權重，就可以決定出正確的方程式。
也就可以讓 f(貓的照片)="這是一隻喵!"

但為了達到這個目標，還必須先思考一些問題:

* 神經網路的架構
* 目標函數的決定(誤差)
* 學習的方法(權重的最佳化)

>這些問題只是基本，可以衍伸出更多難度很深的問題。
我剛接觸深度學習不久，無法涵蓋到很多內容，還請多包涵XD

接下來，我會直接用"**手寫辨識**"來進行說明，也順便為後面的實作鋪陳~

### 神經網路的架構:
* 決定輸入:
在決定輸入前，我們先來看看我們的問題。
下面是一張手寫A以及一張手寫B(160pix * 160pix)，用小畫家畫的:
![](https://i.imgur.com/Zo48Wtd.jpg)
![](https://i.imgur.com/gqt69OY.jpg)
我希望能夠用深度學習來分辨一張圖片是A還是B。

但是，要如何把它轉換成神經網路可以處理的格式呢?
>也就是[K1,K2,K3,K4...]

我把圖片切成 10*10 總共100格，然後用一維陣列去表示它。
白色的格子是1，黑色的格子是0。

![](https://i.imgur.com/RRxXLJM.jpg)
* 決定輸出:

在完美狀況下，輸出就是[1,0] (代表A)或[0,1] (代表B)。
但是sigmoid是一個0~1的函數，所以選最大的當1、其餘當0
>[0.78 , 0.3]=[1,0]

* 決定有幾個神經層在輸入層以及輸出層之間:

中間的神經層被稱為隱藏層。通常隱藏層數量越多就越能辨識更複雜的問題。
我們的問題很簡單，所以我只做了2個隱藏層(每層200個神經元)。

於是乎，模型的大概就如下圖所示:
![](https://i.imgur.com/vj9IZUS.png)
### 目標函數的決定(誤差):

目標函數就是誤差函數，通常會把算出的結果跟我們想得到的目標相減，
然後平方加總(也有別的公式)，有點像是在算標準差的感覺。
![](https://i.imgur.com/z2nwQfN.jpg)
(會多乘上2分之1是為了後面方便計算)
>得到的結果:
>   [0.78 , 0.3]
>想要得到的結果:
>   [1 , 0]
>2倍誤差= (1-0.78)^2^+(0-0.3)^2^ = 0.1384

這個函數除了可以決定我們找到函數的好壞(誤差越小越好)，也可以用來幫助我們修正函數的權重(利用倒傳遞演算法)。
### 學習的方法(權重的最佳化)

這部分也是深度學習的精隨，主要會介紹"倒傳遞演算法"。(backpropagation)

首先，我們來談談如何找到方程式的正確權重。

我們當然可以一個數字一個數字慢慢代換到權重參數裡，但是這樣實在是太花時間了，畢竟我們的參數可能有上千個。

所以與其慢慢試，不如一開始就直接隨機選取權重，之後再透過慢慢修正錯誤的權重參數來找到正確的方程式。
>有些研究為了加速學習效率，權重並不是隨機選取。
>但是隨機選取的效果，依然可以達到我們的期望

在這次的實作中，權重的隨機值只會是-1 、 0 或1。
>你可以試著調整隨機範圍，太大的話會很難最佳化，因為sigmoid函數很容易就會跑到0或1的位置，你也可以採用別的激活函數，就不會有這個問題。

那我們要如何修正錯誤的參數呢?
我們來看看下面的圖形:
![](https://i.imgur.com/mH5Mq2f.jpg)
假設這是我的誤差函數對某"一個"權重參數的圖形。
>參數等於2時，誤差為4(紅點)
>參數等於-1時，誤差為1(藍點)

考慮我們在紅點斜率是大於0的。
而藍點斜率則是小於0。

如果我們要讓誤差為0，在紅點我們必須讓參數值減少。
在藍點則讓參數值增加

也就是在我們知道斜率(該點對參數微分)的狀況下，讓參數朝斜率的"反向"調整，就可以讓誤差最小。

所以我們的問題就變成了必須對誤差函數求出每個權重參數的導數(斜率)
但是我們的方程式有上千個參數，而且還一層一層疊加，那我們要如何求導呢?

這個問題有人已經幫我們解決了，也就是倒傳遞演算法。
利用以下四條公式:
![](https://i.imgur.com/FEZt9qj.jpg)
>這個部份我會建議看這篇文章的推導過程:
>當然，你也可以把公式當作已知
>因為不是每個人都念數學系XD
>[backpropagation algorithm](http://neuralnetworksanddeeplearning.com/chap2.html)

